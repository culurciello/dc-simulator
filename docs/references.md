# References

#### Analysis

https://newsletter.semianalysis.com/p/another-giant-leap-the-rubin-cpx-specialized-accelerator-rack

#### NVIDIA


https://developer.nvidia.com/blog/nvidia-grace-hopper-superchip-architecture-in-depth/

https://developer.nvidia.com/blog/simplifying-gpu-programming-for-hpc-with-the-nvidia-grace-hopper-superchip/


#### Transformers and KV cache

https://ig.ft.com/generative-ai/

https://poloclub.github.io/transformer-explainer/

https://dl.acm.org/doi/10.1145/3731569.3764815

https://yifanqiao.notion.site/Solve-the-GPU-Cost-Crisis-with-kvcached-289da9d1f4d68034b17bf2774201b141

https://lmcache.ai

https://lmcache.ai/tech_report.pdf

https://peterchng.com/blog/2024/05/01/why-do-llm-input-tokens-cost-less-than-output-tokens/


#### Frameworks:

https://pytorch.org/projects/vllm/

https://onnxruntime.ai/docs/tutorials/accelerate-pytorch/pytorch.html

https://docs.fast.ai


#### Hardware

https://www.anyscale.com/blog/ray-direct-transport-rdma-support-in-ray-core